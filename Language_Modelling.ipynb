{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup \n",
    "\n",
    "Please run the code below to mount drive if you are running on colab.\n",
    "\n",
    "Please ignore if you are running on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/MiniGPT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling and Transformers\n",
    "\n",
    "The project will consist of two broad parts. \n",
    "\n",
    "1. **Baseline Generative Language Model**: We will train a simple Bigram language model on the text data. We will use this model to generate a mini story. \n",
    "2. **Implementing Mini GPT**: We will implement a mini version of the GPT model layer by layer and attempt to train it on the text data. You will then load pretrained weights provided and generate a mini story. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some general instructions \n",
    "\n",
    "1. Please keep the name of layers consistent with what is requested in the `model.py` file for each layer, this helps us test in each function independently. \n",
    "2. Please check to see if the bias is to be set to false or true for all linear layers (it is mentioned in the doc string)\n",
    "3. As a general rule please read the docstring well, it contains information you will need to write the code. \n",
    "4. All configs are defined in `config.py` for the first part. While you are writing the code, do not change the values in the config file since we use them to test. Once you have passed all the tests please feel free to vary the parameter as you please.\n",
    "5. You will need to fill in `train.py` and run it to train the model. If you are running into memory issues please feel free to change the `batch_size` in the `config.py` file. If you are working on Colab please make sure to use the GPU runtime and feel free to copy over the training code to the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy torch tiktoken wandb einops # Install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "12.1\n",
      "90100\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BigramLanguageModel, SingleHeadAttention, MultiHeadAttention, FeedForwardLayer, LayerNorm, TransformerLayer, MiniGPT\n",
    "from config import BigramConfig, MiniGPTConfig\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not provided, download from https://drive.google.com/file/d/1g09qUM9WibdfQVgkj6IAj8K2S3SGwc91/view?usp=sharing\n",
    "path_to_bigram_tester = \"./pretrained_models/bigram_tester.pt\" # Load the bigram model with name bigram_tester.pt\n",
    "path_to_gpt_tester = \"./pretrained_models/minigpt_tester.pt\" # Load the gpt model with name minigpt_tester.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bigram Language Model (10 points)\n",
    "\n",
    "A bigram language model is a type of probabilistic language model that predicts a word given the previous word in the sequence. The model is trained on a text corpus and learns the probability of a word given the previous word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Bigram model (5 points)\n",
    "\n",
    "Please complete the `BigramLanguageModel` class in model.py. We will model a Bigram language model using a simple MLP with one hidden layer. The model will take in the previous word index and output the logits over the vocabulary for the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\UCLA\\239_DeepLearning2\\Project3\\Project3_skeleton\\tests.py:375: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_file, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test implementation for Bigram Language Model\n",
    "model = BigramLanguageModel(BigramConfig)\n",
    "tests.check_bigram(model, path_to_bigram_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Bigram Language Model (2.5 points)\n",
    "\n",
    "Complete the code in `train.py` to train the Bigram language model on the text data. Please provide plots for both the training and validation in the cell below.\n",
    "\n",
    "Some notes on the training process:\n",
    "\n",
    "1. You should be able to train the model slowly on your local machine.\n",
    "2. Training it on Colab will help with speed.\n",
    "3.  <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You should see it saturate to a value close to around 5-6 but as long as you see it decreasing then saturating you should be good.\n",
    "4. Please log the loss curves either on wandb, tensorboard or any other logger of your choice and please attach them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: 3.27M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlyuchenandy\u001b[0m (\u001b[33mlyuchenandy-ucla\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\UCLA\\239_DeepLearning2\\Project3\\Project3_skeleton\\wandb\\run-20250511_161751-bw68vut3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lyuchenandy-ucla/dl2_proj3/runs/bw68vut3' target=\"_blank\">playful-yogurt-34</a></strong> to <a href='https://wandb.ai/lyuchenandy-ucla/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lyuchenandy-ucla/dl2_proj3' target=\"_blank\">https://wandb.ai/lyuchenandy-ucla/dl2_proj3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lyuchenandy-ucla/dl2_proj3/runs/bw68vut3' target=\"_blank\">https://wandb.ai/lyuchenandy-ucla/dl2_proj3/runs/bw68vut3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss: 10.8249 Eval Loss: 10.8250\n",
      "Iteration 100, Train Loss: 10.8146 Eval Loss: 10.8133\n",
      "Iteration 200, Train Loss: 10.7972 Eval Loss: 10.7998\n",
      "Iteration 300, Train Loss: 10.7846 Eval Loss: 10.7845\n",
      "Iteration 400, Train Loss: 10.7641 Eval Loss: 10.7661\n",
      "Iteration 500, Train Loss: 10.7352 Eval Loss: 10.7434\n",
      "Iteration 600, Train Loss: 10.7035 Eval Loss: 10.7143\n",
      "Iteration 700, Train Loss: 10.6805 Eval Loss: 10.6784\n",
      "Iteration 800, Train Loss: 10.6162 Eval Loss: 10.6345\n",
      "Iteration 900, Train Loss: 10.5917 Eval Loss: 10.5840\n",
      "Iteration 1000, Train Loss: 10.5249 Eval Loss: 10.5275\n",
      "Iteration 1100, Train Loss: 10.4836 Eval Loss: 10.4637\n",
      "Iteration 1200, Train Loss: 10.3988 Eval Loss: 10.3949\n",
      "Iteration 1300, Train Loss: 10.4027 Eval Loss: 10.3216\n",
      "Iteration 1400, Train Loss: 10.2571 Eval Loss: 10.2448\n",
      "Iteration 1500, Train Loss: 10.1737 Eval Loss: 10.1637\n",
      "Iteration 1600, Train Loss: 10.0883 Eval Loss: 10.0779\n",
      "Iteration 1700, Train Loss: 10.0480 Eval Loss: 9.9912\n",
      "Iteration 1800, Train Loss: 10.1137 Eval Loss: 9.9003\n",
      "Iteration 1900, Train Loss: 9.6674 Eval Loss: 9.8116\n",
      "Iteration 2000, Train Loss: 9.9475 Eval Loss: 9.7207\n",
      "Iteration 2100, Train Loss: 9.7107 Eval Loss: 9.6294\n",
      "Iteration 2200, Train Loss: 9.6782 Eval Loss: 9.5396\n",
      "Iteration 2300, Train Loss: 9.5366 Eval Loss: 9.4469\n",
      "Iteration 2400, Train Loss: 9.6806 Eval Loss: 9.3567\n",
      "Iteration 2500, Train Loss: 9.3644 Eval Loss: 9.2696\n",
      "Iteration 2600, Train Loss: 9.4624 Eval Loss: 9.1812\n",
      "Iteration 2700, Train Loss: 9.2418 Eval Loss: 9.0935\n",
      "Iteration 2800, Train Loss: 9.4810 Eval Loss: 9.0096\n",
      "Iteration 2900, Train Loss: 9.1881 Eval Loss: 8.9266\n",
      "Iteration 3000, Train Loss: 9.2434 Eval Loss: 8.8440\n",
      "Iteration 3100, Train Loss: 8.9733 Eval Loss: 8.7648\n",
      "Iteration 3200, Train Loss: 8.9024 Eval Loss: 8.6868\n",
      "Iteration 3300, Train Loss: 9.2205 Eval Loss: 8.6104\n",
      "Iteration 3400, Train Loss: 8.9434 Eval Loss: 8.5392\n",
      "Iteration 3500, Train Loss: 8.6988 Eval Loss: 8.4657\n",
      "Iteration 3600, Train Loss: 8.2315 Eval Loss: 8.3954\n",
      "Iteration 3700, Train Loss: 9.2321 Eval Loss: 8.3241\n",
      "Iteration 3800, Train Loss: 8.7180 Eval Loss: 8.2567\n",
      "Iteration 3900, Train Loss: 8.5271 Eval Loss: 8.1930\n",
      "Iteration 4000, Train Loss: 8.7776 Eval Loss: 8.1316\n",
      "Iteration 4100, Train Loss: 8.6717 Eval Loss: 8.0707\n",
      "Iteration 4200, Train Loss: 8.5304 Eval Loss: 8.0093\n",
      "Iteration 4300, Train Loss: 8.3698 Eval Loss: 7.9533\n",
      "Iteration 4400, Train Loss: 8.3967 Eval Loss: 7.8981\n",
      "Iteration 4500, Train Loss: 8.1038 Eval Loss: 7.8431\n",
      "Iteration 4600, Train Loss: 7.6748 Eval Loss: 7.7918\n",
      "Iteration 4700, Train Loss: 8.1669 Eval Loss: 7.7429\n",
      "Iteration 4800, Train Loss: 8.8001 Eval Loss: 7.6936\n",
      "Iteration 4900, Train Loss: 8.4134 Eval Loss: 7.6500\n",
      "Iteration 5000, Train Loss: 8.0546 Eval Loss: 7.6045\n",
      "Iteration 5100, Train Loss: 8.1146 Eval Loss: 7.5630\n",
      "Iteration 5200, Train Loss: 8.1547 Eval Loss: 7.5216\n",
      "Iteration 5300, Train Loss: 7.7966 Eval Loss: 7.4818\n",
      "Iteration 5400, Train Loss: 8.2570 Eval Loss: 7.4428\n",
      "Iteration 5500, Train Loss: 7.6934 Eval Loss: 7.4092\n",
      "Iteration 5600, Train Loss: 7.8391 Eval Loss: 7.3725\n",
      "Iteration 5700, Train Loss: 7.8439 Eval Loss: 7.3389\n",
      "Iteration 5800, Train Loss: 7.5295 Eval Loss: 7.3064\n",
      "Iteration 5900, Train Loss: 8.1796 Eval Loss: 7.2717\n",
      "Iteration 6000, Train Loss: 8.1324 Eval Loss: 7.2415\n",
      "Iteration 6100, Train Loss: 7.9656 Eval Loss: 7.2104\n",
      "Iteration 6200, Train Loss: 7.7386 Eval Loss: 7.1797\n",
      "Iteration 6300, Train Loss: 7.9434 Eval Loss: 7.1542\n",
      "Iteration 6400, Train Loss: 7.6381 Eval Loss: 7.1246\n",
      "Iteration 6500, Train Loss: 7.9834 Eval Loss: 7.0964\n",
      "Iteration 6600, Train Loss: 7.6424 Eval Loss: 7.0692\n",
      "Iteration 6700, Train Loss: 8.4573 Eval Loss: 7.0434\n",
      "Iteration 6800, Train Loss: 7.9967 Eval Loss: 7.0181\n",
      "Iteration 6900, Train Loss: 7.4048 Eval Loss: 6.9925\n",
      "Iteration 7000, Train Loss: 7.7950 Eval Loss: 6.9694\n",
      "Iteration 7100, Train Loss: 7.8932 Eval Loss: 6.9463\n",
      "Iteration 7200, Train Loss: 7.6115 Eval Loss: 6.9253\n",
      "Iteration 7300, Train Loss: 7.1580 Eval Loss: 6.9025\n",
      "Iteration 7400, Train Loss: 7.8431 Eval Loss: 6.8785\n",
      "Iteration 7500, Train Loss: 6.8300 Eval Loss: 6.8599\n",
      "Iteration 7600, Train Loss: 8.0076 Eval Loss: 6.8407\n",
      "Iteration 7700, Train Loss: 7.3861 Eval Loss: 6.8183\n",
      "Iteration 7800, Train Loss: 7.3036 Eval Loss: 6.7979\n",
      "Iteration 7900, Train Loss: 6.7987 Eval Loss: 6.7814\n",
      "Iteration 8000, Train Loss: 7.5896 Eval Loss: 6.7621\n",
      "Iteration 8100, Train Loss: 7.1631 Eval Loss: 6.7449\n",
      "Iteration 8200, Train Loss: 7.3111 Eval Loss: 6.7259\n",
      "Iteration 8300, Train Loss: 7.1426 Eval Loss: 6.7086\n",
      "Iteration 8400, Train Loss: 8.2099 Eval Loss: 6.6899\n",
      "Iteration 8500, Train Loss: 7.7169 Eval Loss: 6.6743\n",
      "Iteration 8600, Train Loss: 8.2464 Eval Loss: 6.6585\n",
      "Iteration 8700, Train Loss: 7.0897 Eval Loss: 6.6420\n",
      "Iteration 8800, Train Loss: 7.6961 Eval Loss: 6.6269\n",
      "Iteration 8900, Train Loss: 7.4899 Eval Loss: 6.6112\n",
      "Iteration 9000, Train Loss: 7.0124 Eval Loss: 6.5975\n",
      "Iteration 9100, Train Loss: 6.5931 Eval Loss: 6.5804\n",
      "Iteration 9200, Train Loss: 7.9585 Eval Loss: 6.5675\n",
      "Iteration 9300, Train Loss: 7.4021 Eval Loss: 6.5530\n",
      "Iteration 9400, Train Loss: 7.0433 Eval Loss: 6.5392\n",
      "Iteration 9500, Train Loss: 6.9422 Eval Loss: 6.5273\n",
      "Iteration 9600, Train Loss: 7.5968 Eval Loss: 6.5108\n",
      "Iteration 9700, Train Loss: 7.3903 Eval Loss: 6.4995\n",
      "Iteration 9800, Train Loss: 7.6185 Eval Loss: 6.4854\n",
      "Iteration 9900, Train Loss: 7.3778 Eval Loss: 6.4733\n",
      "Iteration 10000, Train Loss: 7.1822 Eval Loss: 6.4589\n",
      "Iteration 10100, Train Loss: 6.7000 Eval Loss: 6.4489\n",
      "Iteration 10200, Train Loss: 7.3869 Eval Loss: 6.4382\n",
      "Iteration 10300, Train Loss: 6.7524 Eval Loss: 6.4253\n",
      "Iteration 10400, Train Loss: 7.2153 Eval Loss: 6.4131\n",
      "Iteration 10500, Train Loss: 7.9936 Eval Loss: 6.4033\n",
      "Iteration 10600, Train Loss: 6.8913 Eval Loss: 6.3918\n",
      "Iteration 10700, Train Loss: 6.8645 Eval Loss: 6.3814\n",
      "Iteration 10800, Train Loss: 7.5152 Eval Loss: 6.3723\n",
      "Iteration 10900, Train Loss: 7.5081 Eval Loss: 6.3621\n",
      "Iteration 11000, Train Loss: 7.4218 Eval Loss: 6.3520\n",
      "Iteration 11100, Train Loss: 6.8717 Eval Loss: 6.3385\n",
      "Iteration 11200, Train Loss: 6.6593 Eval Loss: 6.3293\n",
      "Iteration 11300, Train Loss: 7.5188 Eval Loss: 6.3190\n",
      "Iteration 11400, Train Loss: 6.8681 Eval Loss: 6.3098\n",
      "Iteration 11500, Train Loss: 6.5699 Eval Loss: 6.3007\n",
      "Iteration 11600, Train Loss: 7.0647 Eval Loss: 6.2931\n",
      "Iteration 11700, Train Loss: 7.0426 Eval Loss: 6.2838\n",
      "Iteration 11800, Train Loss: 6.6802 Eval Loss: 6.2741\n",
      "Iteration 11900, Train Loss: 6.6519 Eval Loss: 6.2641\n",
      "Iteration 12000, Train Loss: 6.7034 Eval Loss: 6.2564\n",
      "Iteration 12100, Train Loss: 7.3485 Eval Loss: 6.2471\n",
      "Iteration 12200, Train Loss: 6.9995 Eval Loss: 6.2382\n",
      "Iteration 12300, Train Loss: 6.8485 Eval Loss: 6.2313\n",
      "Iteration 12400, Train Loss: 7.2362 Eval Loss: 6.2219\n",
      "Iteration 12500, Train Loss: 6.5526 Eval Loss: 6.2155\n",
      "Iteration 12600, Train Loss: 6.8779 Eval Loss: 6.2068\n",
      "Iteration 12700, Train Loss: 7.1003 Eval Loss: 6.2010\n",
      "Iteration 12800, Train Loss: 6.6235 Eval Loss: 6.1909\n",
      "Iteration 12900, Train Loss: 6.2651 Eval Loss: 6.1836\n",
      "Iteration 13000, Train Loss: 6.5145 Eval Loss: 6.1760\n",
      "Iteration 13100, Train Loss: 6.2680 Eval Loss: 6.1685\n",
      "Iteration 13200, Train Loss: 7.1730 Eval Loss: 6.1619\n",
      "Iteration 13300, Train Loss: 6.5164 Eval Loss: 6.1532\n",
      "Iteration 13400, Train Loss: 6.9879 Eval Loss: 6.1472\n",
      "Iteration 13500, Train Loss: 7.6420 Eval Loss: 6.1406\n",
      "Iteration 13600, Train Loss: 7.4823 Eval Loss: 6.1343\n",
      "Iteration 13700, Train Loss: 7.4603 Eval Loss: 6.1263\n",
      "Iteration 13800, Train Loss: 6.9380 Eval Loss: 6.1165\n",
      "Iteration 13900, Train Loss: 7.0976 Eval Loss: 6.1143\n",
      "Iteration 14000, Train Loss: 6.6535 Eval Loss: 6.1082\n",
      "Iteration 14100, Train Loss: 7.0897 Eval Loss: 6.1000\n",
      "Iteration 14200, Train Loss: 6.7318 Eval Loss: 6.0940\n",
      "Iteration 14300, Train Loss: 6.8420 Eval Loss: 6.0873\n",
      "Iteration 14400, Train Loss: 6.3961 Eval Loss: 6.0823\n",
      "Iteration 14500, Train Loss: 6.7518 Eval Loss: 6.0747\n",
      "Iteration 14600, Train Loss: 6.9949 Eval Loss: 6.0700\n",
      "Iteration 14700, Train Loss: 7.3024 Eval Loss: 6.0629\n",
      "Iteration 14800, Train Loss: 6.2764 Eval Loss: 6.0588\n",
      "Iteration 14900, Train Loss: 5.6643 Eval Loss: 6.0514\n",
      "Iteration 15000, Train Loss: 6.5947 Eval Loss: 6.0468\n",
      "Iteration 15100, Train Loss: 6.3693 Eval Loss: 6.0390\n",
      "Iteration 15200, Train Loss: 6.6979 Eval Loss: 6.0344\n",
      "Iteration 15300, Train Loss: 6.0958 Eval Loss: 6.0264\n",
      "Iteration 15400, Train Loss: 6.9696 Eval Loss: 6.0224\n",
      "Iteration 15500, Train Loss: 6.2092 Eval Loss: 6.0162\n",
      "Iteration 15600, Train Loss: 6.4842 Eval Loss: 6.0134\n",
      "Iteration 15700, Train Loss: 6.2661 Eval Loss: 6.0056\n",
      "Iteration 15800, Train Loss: 7.0385 Eval Loss: 6.0032\n",
      "Iteration 15900, Train Loss: 6.5335 Eval Loss: 5.9977\n",
      "Iteration 16000, Train Loss: 6.0494 Eval Loss: 5.9908\n",
      "Iteration 16100, Train Loss: 6.3674 Eval Loss: 5.9849\n",
      "Iteration 16200, Train Loss: 6.7154 Eval Loss: 5.9808\n",
      "Iteration 16300, Train Loss: 6.6046 Eval Loss: 5.9720\n",
      "Iteration 16400, Train Loss: 6.6715 Eval Loss: 5.9694\n",
      "Iteration 16500, Train Loss: 7.4190 Eval Loss: 5.9645\n",
      "Iteration 16600, Train Loss: 7.4703 Eval Loss: 5.9594\n",
      "Iteration 16700, Train Loss: 7.0946 Eval Loss: 5.9530\n",
      "Iteration 16800, Train Loss: 6.6257 Eval Loss: 5.9508\n",
      "Iteration 16900, Train Loss: 7.4752 Eval Loss: 5.9449\n",
      "Iteration 17000, Train Loss: 6.0851 Eval Loss: 5.9405\n",
      "Iteration 17100, Train Loss: 6.4321 Eval Loss: 5.9349\n",
      "Iteration 17200, Train Loss: 6.8536 Eval Loss: 5.9304\n",
      "Iteration 17300, Train Loss: 6.2638 Eval Loss: 5.9254\n",
      "Iteration 17400, Train Loss: 7.5621 Eval Loss: 5.9197\n",
      "Iteration 17500, Train Loss: 6.9187 Eval Loss: 5.9176\n",
      "Iteration 17600, Train Loss: 6.8031 Eval Loss: 5.9108\n",
      "Iteration 17700, Train Loss: 7.1363 Eval Loss: 5.9043\n",
      "Iteration 17800, Train Loss: 6.7209 Eval Loss: 5.8989\n",
      "Iteration 17900, Train Loss: 6.3880 Eval Loss: 5.8936\n",
      "Iteration 18000, Train Loss: 7.2215 Eval Loss: 5.8900\n",
      "Iteration 18100, Train Loss: 6.4036 Eval Loss: 5.8838\n",
      "Iteration 18200, Train Loss: 5.9372 Eval Loss: 5.8835\n",
      "Iteration 18300, Train Loss: 6.7914 Eval Loss: 5.8757\n",
      "Iteration 18400, Train Loss: 6.8028 Eval Loss: 5.8715\n",
      "Iteration 18500, Train Loss: 7.1033 Eval Loss: 5.8661\n",
      "Iteration 18600, Train Loss: 7.0511 Eval Loss: 5.8618\n",
      "Iteration 18700, Train Loss: 6.0012 Eval Loss: 5.8569\n",
      "Iteration 18800, Train Loss: 6.3652 Eval Loss: 5.8506\n",
      "Iteration 18900, Train Loss: 7.3431 Eval Loss: 5.8485\n",
      "Iteration 19000, Train Loss: 6.7913 Eval Loss: 5.8444\n",
      "Iteration 19100, Train Loss: 6.5727 Eval Loss: 5.8373\n",
      "Iteration 19200, Train Loss: 6.6203 Eval Loss: 5.8336\n",
      "Iteration 19300, Train Loss: 6.5534 Eval Loss: 5.8293\n",
      "Iteration 19400, Train Loss: 7.3012 Eval Loss: 5.8270\n",
      "Iteration 19500, Train Loss: 6.2832 Eval Loss: 5.8189\n",
      "Iteration 19600, Train Loss: 6.2529 Eval Loss: 5.8123\n",
      "Iteration 19700, Train Loss: 6.5536 Eval Loss: 5.8113\n",
      "Iteration 19800, Train Loss: 6.4225 Eval Loss: 5.8056\n",
      "Iteration 19900, Train Loss: 6.0372 Eval Loss: 5.8041\n",
      "Iteration 20000, Train Loss: 6.1157 Eval Loss: 5.7968\n",
      "Iteration 20100, Train Loss: 6.1971 Eval Loss: 5.7933\n",
      "Iteration 20200, Train Loss: 6.1693 Eval Loss: 5.7891\n",
      "Iteration 20300, Train Loss: 7.1677 Eval Loss: 5.7837\n",
      "Iteration 20400, Train Loss: 6.6711 Eval Loss: 5.7783\n",
      "Iteration 20500, Train Loss: 6.1589 Eval Loss: 5.7712\n",
      "Iteration 20600, Train Loss: 7.0855 Eval Loss: 5.7684\n",
      "Iteration 20700, Train Loss: 7.1162 Eval Loss: 5.7623\n",
      "Iteration 20800, Train Loss: 6.2212 Eval Loss: 5.7593\n",
      "Iteration 20900, Train Loss: 6.2754 Eval Loss: 5.7537\n",
      "Iteration 21000, Train Loss: 6.8737 Eval Loss: 5.7518\n",
      "Iteration 21100, Train Loss: 5.7137 Eval Loss: 5.7482\n",
      "Iteration 21200, Train Loss: 6.7923 Eval Loss: 5.7393\n",
      "Iteration 21300, Train Loss: 6.5062 Eval Loss: 5.7342\n",
      "Iteration 21400, Train Loss: 6.1061 Eval Loss: 5.7322\n",
      "Iteration 21500, Train Loss: 6.2791 Eval Loss: 5.7266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\UCLA\\239_DeepLearning2\\Project3\\Project3_skeleton\\train.py:132\u001b[0m, in \u001b[0;36msolver\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m### ======== TODO : START ========= ###\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Compute the evaluation loss on the eval dataset.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# print(\"len(eval_dataloader) = \", len(eval_dataloader))\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, (context, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(eval_dataloader):\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_dataloader):\n\u001b[0;32m    135\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lyuch\\anaconda3\\envs\\emg2qwerty\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\lyuch\\anaconda3\\envs\\emg2qwerty\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lyuch\\anaconda3\\envs\\emg2qwerty\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1402\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1404\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\lyuch\\anaconda3\\envs\\emg2qwerty\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lyuch\\anaconda3\\envs\\emg2qwerty\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\lyuch\\anaconda3\\envs\\emg2qwerty\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "solver(model_name=\"bigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Valid Plots\n",
    "\n",
    "\n",
    "** Show the training and validation loss plots **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (2.5 points)\n",
    "\n",
    "Complete the code in the `generate` method of the Bigram class and generate a mini story using the trained Bigram language model. The model will take in the previous word index and output the next word index.\n",
    "\n",
    "Start with the following seed sentence: \n",
    "    \n",
    "    `\"once upon a time\"`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify the path to your trained model\n",
    "model_path = \"models/bigram/best_model.pt\"\n",
    "model = BigramLanguageModel(BigramConfig)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation and Analysis\n",
    "\n",
    "Please answer the following questions. \n",
    "\n",
    "1. What can we say about the generated text in terms of grammar and coherence? \n",
    "2. What are the limitations of the Bigram language model?\n",
    "3. If the model is scaled with more parameters do you expect the bigram model to get substantially better? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GPT (90 points)\n",
    "\n",
    "We will implement a decoder style transformer model like we discussed in lecture, which is a scaled down version of the [GPT model](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf). \n",
    "\n",
    "All the model components follow directly from the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. The only difference is we will use prenormalization and learnt positional embeddings instead of fixed ones.\n",
    "\n",
    "We will now implement each layer step by step checking if it is implemented correctly in the process. We will finally put together all our layers to get a fully fledged GPT model. \n",
    "\n",
    "<span style=\"color:red\">Later layers might depend on previous layers so please make sure to check the previous layers before moving on to the next one.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Head Causal Attention (20 points)\n",
    "\n",
    "We will first implement the single head causal attention layer. This layer is the same as the scaled dot product attention layer but with a causal mask to prevent the model from looking into the future.\n",
    "\n",
    "Recall that Each head has a Key, Query and Value Matrix and the scaled dot product attention is calculated as : \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation}\n",
    "\n",
    "where $d_k$ is the dimension of the key matrix.\n",
    "\n",
    "Figure below from the original paper shows how the layer is to be implemented.\n",
    "\n",
    "![image](./Images/Single_Head.png)\n",
    "\n",
    "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `SingleHeadAttention` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.embed_dim//4, MiniGPTConfig.embed_dim//4) # configs are set as such for testing do not modify\n",
    "tests.check_singleheadattention(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention (10 points)\n",
    "\n",
    "Now that we have a single head working, we will now scale this across multiple heads, remember that with multihead attention we compute perform head number of parallel attention operations. We then concatenate the outputs of these parallel attention operations and project them back to the desired dimension using an output linear layer.\n",
    "\n",
    "Figure below from the original paper shows how the layer is to be implemented.\n",
    "\n",
    "![image](./Images/MultiHead.png)\n",
    "\n",
    "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `MultiHeadAttention` class in `model.py` using the `SingleHeadAttention` class implemented earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
    "tests.check_multiheadattention(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Layer (5 points)\n",
    "\n",
    "As discussed in lecture, the attention layer is completely linear, in order to add some non-linearity we add a feed forward layer. The feed forward layer is a simple two layer MLP with a GeLU activation in between.\n",
    "\n",
    "Please complete the `FeedForwardLayer` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForwardLayer(MiniGPTConfig.embed_dim)\n",
    "tests.check_feedforward(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm (10 points)\n",
    "\n",
    "We will now implement the layer normalization layer. Layernorm is used across the model to normalize the activations of the previous layer. Recall that the equation for layernorm is given as:\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot \\gamma + \\beta\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "With the learnable parameters $\\gamma$ and $\\beta$. \n",
    "\n",
    "Remember that unlike batchnorm we compute statistics across the feature dimension and not the batch dimension, hence we do not need to keep track of running averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `LayerNorm` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LayerNorm(MiniGPTConfig.embed_dim)\n",
    "tests.check_layernorm(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Layer (15 points)\n",
    "\n",
    "We have now implemented all the components of the transformer layer. We will now put it all together to create a transformer layer. The transformer layer consists of a multi head attention layer, a feed forward layer and two layer norm layers.\n",
    "\n",
    "Please use the following order for each component (Varies slightly from the original attention paper):\n",
    "1. LayerNorm\n",
    "2. MultiHeadAttention\n",
    "3. LayerNorm\n",
    "4. FeedForwardLayer\n",
    "\n",
    "Remember that the transformer layer also has residual connections around each sublayer.\n",
    "\n",
    "The below figure shows the structure of the transformer layer you are required to implement.\n",
    "\n",
    "![prenorm_transformer](./Images/Prenorm.png)\n",
    "\n",
    "Image Credit : [CogView](https://arxiv.org/pdf/2105.13290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `TransformerLayer` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  TransformerLayer(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
    "tests.check_transformer(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together : MiniGPT (15 points)\n",
    "\n",
    "We are now ready to put all our layers together to build our own MiniGPT! \n",
    "\n",
    "The MiniGPT model consists of an embedding layer, a positional encoding layer and a stack of transformer layers. The output of the transformer layer is passed through a linear layer (called head) to get the final output logits. Note that in our implementation we will use [weight tying](https://arxiv.org/abs/1608.05859) between the embedding layer and the final linear layer. This allows us to save on parameters and also helps in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `MiniGPT` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniGPT(MiniGPTConfig)\n",
    "tests.check_miniGPT(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at training the model (5 points)\n",
    "\n",
    "We will now attempt to train the model on the text data. We will use the same text data as before. If needed, you can scale down the model parameters in the config file to a smaller value to make training feasible. \n",
    "\n",
    "Use the same training script we built for the Bigram model to train the MiniGPT model. If you implemented it correctly it should work just out of the box!\n",
    "\n",
    "**NOTE** : We will not be able to train the model to completion in this assignment. Unfortunately, without access to a relatively powerful GPU, training a large enough model to see good generation is not feasible. However, you should be able to see the loss decreasing over time. <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You do not need to run this for more than 5000 iterations or 1 hour of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver(model_name=\"minigpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Valid Plots\n",
    "\n",
    "\n",
    "** Show the training and validation loss plots **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (5 points)\n",
    "\n",
    "\n",
    "Perform generation with the MiniGPT model that you trained. After that, copy over the generation function you used for the Bigram model and generate a mini story using the same seed sentence. \n",
    "\n",
    "    `\"once upon a time\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify the path to your trained model\n",
    "model_path = None\n",
    "model = MiniGPT(MiniGPTConfig)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following questions. \n",
    "\n",
    "1. What can we say about the generated text in terms of grammar and coherence? \n",
    "2. If the model is scaled with more parameters do you expect the GPT model to get substantially better? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up the model (5 points)\n",
    "\n",
    "To show that scale indeed will help the model learn we have trained a scaled up version of the model you just implemented. We will load the weights of this model and generate a mini story using the same seed sentence. Note that if you have implemented the model correctly just scaling the parameters and adding a few bells and whistles to the training script will results in a model like the one we will load now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MiniGPT\n",
    "from config import MiniGPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = \"pretrained_models/best_train_loss_checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(path_to_trained_model, map_location=device) # remove map location if using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the configs for scaled model \n",
    "MiniGPTConfig.context_length = 512\n",
    "MiniGPTConfig.embed_dim = 256\n",
    "MiniGPTConfig.num_heads = 16\n",
    "MiniGPTConfig.num_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from checkpoint\n",
    "model = MiniGPT(MiniGPTConfig)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (5 points)\n",
    "\n",
    "The following are some open ended questions that you can attempt if you have time. Feel free to propose your own as well if you have an interesting idea. \n",
    "\n",
    "1. The model we have implemented is a decoder only model. Can you implement the encoder part as well? This should not be too hard to do since most of the layers are already implemented.\n",
    "2. What are some improvements we can add to the training script to make training more efficient and faster? Can you concretely show that the improvements you made help in training the model better?\n",
    "3. Can you implement a beam search decoder to generate the text instead of greedy decoding? Does this help in generating better text?\n",
    "4. Can you further optimize the model architecture? For example, can you implement [Multi Query Attention](https://arxiv.org/abs/1911.02150) or [Grouped Query Attention](https://arxiv.org/pdf/2305.13245) to improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emg2qwerty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
